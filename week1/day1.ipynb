{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Website Analysis Demo\n",
    "## Multi-Agent System for Marketing Team Autonomy\n",
    "\n",
    "This demo showcases a working multi-agent system that analyzes websites and provides actionable design recommendations for Marketing teams.\n",
    "\n",
    "### Demo Flow:\n",
    "1. **Web Acquisition Agent** - Fetches and analyzes website structure\n",
    "2. **Analysis Agent** - AI-powered analysis of design patterns and UX issues  \n",
    "3. **Design Suggestion Agent** - Generates Marketing-focused improvement recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "from playwright.sync_api import sync_playwright\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Anthropic client\n",
    "try:\n",
    "    client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Anthropic client initialization failed: {e}\")\n",
    "    client = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Web Acquisition Agent (Auto-Port + Fixed) initialized!\n",
      "\n",
      "üí° Note: Static server works for captured React content!\n",
      "   ‚úÖ Visual layout preserved (perfect for Marketing edits)\n",
      "   ‚ùå Interactive features disabled (buttons, forms won't work)\n",
      "   üéØ Ideal for HTML/CSS modifications and design changes\n"
     ]
    }
   ],
   "source": [
    "## ü§ñ Agent 1: Web Acquisition Agent (Auto-Port Selection)\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import urllib.parse\n",
    "import threading\n",
    "import subprocess\n",
    "import concurrent.futures\n",
    "import time\n",
    "import socket\n",
    "\n",
    "class WebAcquisitionAgent:\n",
    "    \"\"\"Web Acquisition Agent with HTML structure preservation - Thread isolated solution\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_dir = Path('../sites')\n",
    "        self.base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    def fetch_website(self, url):\n",
    "        \"\"\"Fetch website with full HTML structure and assets for local editing\"\"\"\n",
    "        print(f\"üîç Web Acquisition Agent: Capturing {url} with full structure\")\n",
    "        \n",
    "        # Create site-specific directory\n",
    "        site_name = url.replace('https://', '').replace('http://', '').replace('www.', '').replace('/', '_')\n",
    "        site_dir = self.base_dir / site_name\n",
    "        site_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            # Run Playwright in separate thread to completely avoid asyncio conflicts\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
    "                future = executor.submit(self._scrape_in_thread, url, str(site_dir))\n",
    "                result = future.result(timeout=120)  # 2 minute timeout\n",
    "            \n",
    "            if 'error' in result:\n",
    "                print(f\"‚ùå Error fetching website: {result['error']}\")\n",
    "                return None\n",
    "            \n",
    "            # Save the complete HTML file\n",
    "            html_path = site_dir / 'index.html'\n",
    "            with open(html_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(result['html_content'])\n",
    "            \n",
    "            # Save analysis data\n",
    "            analysis_path = site_dir / 'analysis.json'\n",
    "            with open(analysis_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"‚úÖ Website captured with structure at: {site_dir}\")\n",
    "            print(f\"   üìÑ HTML: {html_path}\")\n",
    "            print(f\"   üìä Analysis: {analysis_path}\")\n",
    "            print(f\"   üé® Assets: {len(result['assets']['css'])} CSS, {len(result['assets']['js'])} JS, {len(result['assets']['images'])} images\")\n",
    "            \n",
    "            return {\n",
    "                'analysis': result,\n",
    "                'html_path': str(html_path),\n",
    "                'site_dir': str(site_dir),\n",
    "                'assets': result['assets']\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in website capture: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _scrape_in_thread(self, url, output_dir):\n",
    "        \"\"\"Run Playwright scraping in completely isolated thread\"\"\"\n",
    "        try:\n",
    "            # Import and run sync Playwright in thread (no asyncio involvement)\n",
    "            from playwright.sync_api import sync_playwright\n",
    "            \n",
    "            with sync_playwright() as p:\n",
    "                browser = p.chromium.launch(headless=True)\n",
    "                page = browser.new_page()\n",
    "                \n",
    "                print(f\"   üåê Navigating to {url}\")\n",
    "                # Navigate and wait for React content to load\n",
    "                page.goto(url, wait_until='networkidle', timeout=60000)\n",
    "                page.wait_for_timeout(3000)  # Extra time for React rendering\n",
    "                \n",
    "                print(f\"   üìÑ Extracting HTML content\")\n",
    "                # Get the complete rendered HTML\n",
    "                html_content = page.content()\n",
    "                \n",
    "                print(f\"   üé® Downloading assets\")\n",
    "                # Download all assets (CSS, JS, images)\n",
    "                assets = self._download_all_assets_sync(page, url, output_dir)\n",
    "                \n",
    "                # Modify HTML to use local assets\n",
    "                modified_html = self._localize_html_assets(html_content, assets, url)\n",
    "                \n",
    "                print(f\"   üìä Analyzing page structure\")\n",
    "                # Extract analysis data\n",
    "                analysis = {\n",
    "                    'url': url,\n",
    "                    'title': page.title(),\n",
    "                    'html_content': modified_html,\n",
    "                    'original_html': html_content,\n",
    "                    'assets': assets,\n",
    "                    'headings': self._extract_headings_sync(page),\n",
    "                    'navigation': self._extract_navigation_sync(page),\n",
    "                    'calls_to_action': self._extract_ctas_sync(page),\n",
    "                    'forms': self._extract_forms_sync(page),\n",
    "                    'images': self._extract_images_data_sync(page),\n",
    "                    'content_sections': self._extract_content_sections_sync(page),\n",
    "                    'meta_description': self._extract_meta_description_sync(page),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                browser.close()\n",
    "                print(f\"   ‚úÖ Thread scraping complete\")\n",
    "                return analysis\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Thread scraping error: {str(e)}\")\n",
    "            return {'error': str(e), 'url': url}\n",
    "\n",
    "    def _download_all_assets_sync(self, page, base_url, output_dir):\n",
    "        \"\"\"Download all CSS, JS, and image assets synchronously\"\"\"\n",
    "        assets = {'css': [], 'js': [], 'images': []}\n",
    "        \n",
    "        # Create asset directories\n",
    "        css_dir = Path(output_dir) / 'css'\n",
    "        js_dir = Path(output_dir) / 'js'\n",
    "        img_dir = Path(output_dir) / 'images'\n",
    "        \n",
    "        css_dir.mkdir(parents=True, exist_ok=True)\n",
    "        js_dir.mkdir(parents=True, exist_ok=True)\n",
    "        img_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Download CSS files\n",
    "        css_elements = page.query_selector_all('link[rel=\"stylesheet\"]')\n",
    "        for i, element in enumerate(css_elements):\n",
    "            href = element.get_attribute('href')\n",
    "            if href:\n",
    "                full_url = urljoin(base_url, href)\n",
    "                local_filename = f'style_{i}.css'\n",
    "                if self._download_asset(full_url, css_dir / local_filename):\n",
    "                    assets['css'].append({\n",
    "                        'original_url': full_url,\n",
    "                        'local_path': f'css/{local_filename}',\n",
    "                        'href': href\n",
    "                    })\n",
    "        \n",
    "        # Download JS files (limit to avoid too many files)\n",
    "        js_elements = page.query_selector_all('script[src]')\n",
    "        for i, element in enumerate(js_elements[:10]):  # Limit JS files\n",
    "            src = element.get_attribute('src')\n",
    "            if src and not src.startswith('data:'):\n",
    "                full_url = urljoin(base_url, src)\n",
    "                local_filename = f'script_{i}.js'\n",
    "                if self._download_asset(full_url, js_dir / local_filename):\n",
    "                    assets['js'].append({\n",
    "                        'original_url': full_url,\n",
    "                        'local_path': f'js/{local_filename}',\n",
    "                        'src': src\n",
    "                    })\n",
    "        \n",
    "        # Download images (limit to avoid too many files)\n",
    "        img_elements = page.query_selector_all('img')\n",
    "        for i, element in enumerate(img_elements[:15]):  # Limit images\n",
    "            src = element.get_attribute('src')\n",
    "            if src and not src.startswith('data:'):\n",
    "                full_url = urljoin(base_url, src)\n",
    "                extension = Path(urllib.parse.urlparse(full_url).path).suffix or '.jpg'\n",
    "                local_filename = f'image_{i}{extension}'\n",
    "                if self._download_asset(full_url, img_dir / local_filename):\n",
    "                    assets['images'].append({\n",
    "                        'original_url': full_url,\n",
    "                        'local_path': f'images/{local_filename}',\n",
    "                        'src': src\n",
    "                    })\n",
    "        \n",
    "        return assets\n",
    "\n",
    "    def _download_asset(self, url, local_path):\n",
    "        \"\"\"Download a single asset file\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10, headers={\n",
    "                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'\n",
    "            })\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(local_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            # Don't print every failed download to reduce noise\n",
    "            return False\n",
    "\n",
    "    def _localize_html_assets(self, html_content, assets, base_url):\n",
    "        \"\"\"Modify HTML to use local asset paths\"\"\"\n",
    "        modified_html = html_content\n",
    "        \n",
    "        # Replace CSS links\n",
    "        for css_asset in assets['css']:\n",
    "            modified_html = modified_html.replace(\n",
    "                f'href=\"{css_asset[\"href\"]}\"',\n",
    "                f'href=\"{css_asset[\"local_path\"]}\"'\n",
    "            )\n",
    "        \n",
    "        # Replace JS sources\n",
    "        for js_asset in assets['js']:\n",
    "            modified_html = modified_html.replace(\n",
    "                f'src=\"{js_asset[\"src\"]}\"',\n",
    "                f'src=\"{js_asset[\"local_path\"]}\"'\n",
    "            )\n",
    "        \n",
    "        # Replace image sources\n",
    "        for img_asset in assets['images']:\n",
    "            modified_html = modified_html.replace(\n",
    "                f'src=\"{img_asset[\"src\"]}\"',\n",
    "                f'src=\"{img_asset[\"local_path\"]}\"'\n",
    "            )\n",
    "        \n",
    "        return modified_html\n",
    "\n",
    "    def _extract_headings_sync(self, page):\n",
    "        \"\"\"Extract all headings with hierarchy\"\"\"\n",
    "        headings = []\n",
    "        for level in range(1, 7):\n",
    "            elements = page.query_selector_all(f'h{level}')\n",
    "            for element in elements:\n",
    "                text = element.text_content()\n",
    "                if text and text.strip():\n",
    "                    headings.append({\n",
    "                        'level': level,\n",
    "                        'text': text.strip(),\n",
    "                        'tag': f'h{level}'\n",
    "                    })\n",
    "        return headings\n",
    "\n",
    "    def _extract_navigation_sync(self, page):\n",
    "        \"\"\"Extract navigation elements\"\"\"\n",
    "        nav_items = []\n",
    "        selectors = ['nav a', 'header a', '.nav a', '.navigation a', '[role=\"navigation\"] a']\n",
    "        \n",
    "        for selector in selectors:\n",
    "            elements = page.query_selector_all(selector)\n",
    "            for element in elements:\n",
    "                text = element.text_content()\n",
    "                href = element.get_attribute('href')\n",
    "                if text and text.strip() and len(text.strip()) < 100:\n",
    "                    nav_items.append({'text': text.strip(), 'href': href})\n",
    "        \n",
    "        return nav_items[:20]\n",
    "\n",
    "    def _extract_ctas_sync(self, page):\n",
    "        \"\"\"Extract call-to-action elements - FIXED\"\"\"\n",
    "        ctas = []\n",
    "        selectors = [\n",
    "            'button', 'a[class*=\"button\"]', 'a[class*=\"btn\"]', \n",
    "            'input[type=\"submit\"]', '.cta', '[class*=\"call-to-action\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            elements = page.query_selector_all(selector)\n",
    "            for element in elements:\n",
    "                text = element.text_content()\n",
    "                href = element.get_attribute('href')\n",
    "                # Fix: Use evaluate to get tag name instead of tag_name property\n",
    "                tag_name = element.evaluate('el => el.tagName.toLowerCase()')\n",
    "                if text and text.strip() and len(text.strip()) < 200:\n",
    "                    ctas.append({\n",
    "                        'text': text.strip(),\n",
    "                        'type': tag_name,\n",
    "                        'href': href\n",
    "                    })\n",
    "        \n",
    "        return ctas[:15]\n",
    "\n",
    "    def _extract_forms_sync(self, page):\n",
    "        \"\"\"Extract form elements\"\"\"\n",
    "        forms = []\n",
    "        form_elements = page.query_selector_all('form')\n",
    "        \n",
    "        for form in form_elements:\n",
    "            inputs = form.query_selector_all('input, textarea, select')\n",
    "            form_data = {\n",
    "                'action': form.get_attribute('action'),\n",
    "                'method': form.get_attribute('method') or 'GET',\n",
    "                'inputs': []\n",
    "            }\n",
    "            \n",
    "            for input_elem in inputs:\n",
    "                form_data['inputs'].append({\n",
    "                    'type': input_elem.get_attribute('type'),\n",
    "                    'name': input_elem.get_attribute('name'),\n",
    "                    'placeholder': input_elem.get_attribute('placeholder')\n",
    "                })\n",
    "            \n",
    "            forms.append(form_data)\n",
    "        \n",
    "        return forms\n",
    "\n",
    "    def _extract_images_data_sync(self, page):\n",
    "        \"\"\"Extract image information\"\"\"\n",
    "        images = []\n",
    "        img_elements = page.query_selector_all('img')\n",
    "        \n",
    "        for img in img_elements[:10]:  # Limit to first 10\n",
    "            src = img.get_attribute('src')\n",
    "            alt = img.get_attribute('alt')\n",
    "            if src:\n",
    "                images.append({\n",
    "                    'src': src,\n",
    "                    'alt': alt,\n",
    "                    'loading': img.get_attribute('loading')\n",
    "                })\n",
    "        \n",
    "        return images\n",
    "\n",
    "    def _extract_content_sections_sync(self, page):\n",
    "        \"\"\"Extract main content sections\"\"\"\n",
    "        sections = []\n",
    "        section_selectors = ['main', 'section', 'article', '.content', '#content']\n",
    "        \n",
    "        for selector in section_selectors:\n",
    "            elements = page.query_selector_all(selector)\n",
    "            for element in elements:\n",
    "                text = element.text_content()\n",
    "                if text and text.strip() and len(text.strip()) > 50:\n",
    "                    sections.append({\n",
    "                        'selector': selector,\n",
    "                        'text_length': len(text),\n",
    "                        'preview': text[:200] + '...' if len(text) > 200 else text\n",
    "                    })\n",
    "        \n",
    "        return sections\n",
    "\n",
    "    def _extract_meta_description_sync(self, page):\n",
    "        \"\"\"Extract meta description\"\"\"\n",
    "        meta_elem = page.query_selector('meta[name=\"description\"]')\n",
    "        return meta_elem.get_attribute('content') if meta_elem else ''\n",
    "\n",
    "    def _find_free_port(self, start_port=8000):\n",
    "        \"\"\"Find an available port starting from start_port\"\"\"\n",
    "        for port in range(start_port, start_port + 100):\n",
    "            try:\n",
    "                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                    s.bind(('localhost', port))\n",
    "                    return port\n",
    "            except OSError:\n",
    "                continue\n",
    "        return None\n",
    "\n",
    "    def start_local_server(self, site_dir, start_port=8000):\n",
    "        \"\"\"Start local server for preview with automatic port selection\"\"\"\n",
    "        \n",
    "        # Find available port\n",
    "        port = self._find_free_port(start_port)\n",
    "        if not port:\n",
    "            print(\"‚ùå No available ports found\")\n",
    "            return None\n",
    "            \n",
    "        def run_server():\n",
    "            try:\n",
    "                print(f\"üåê Starting server on port {port}\")\n",
    "                subprocess.run([\n",
    "                    'python', '-m', 'http.server', str(port)\n",
    "                ], cwd=site_dir, check=True)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"‚ùå Server error: {e}\")\n",
    "            except KeyboardInterrupt:\n",
    "                print(f\"üõë Server on port {port} stopped\")\n",
    "        \n",
    "        server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "        server_thread.start()\n",
    "        \n",
    "        # Give server time to start\n",
    "        time.sleep(1)\n",
    "        \n",
    "        server_url = f\"http://localhost:{port}\"\n",
    "        print(f\"üåê Local server started at {server_url}\")\n",
    "        print(f\"üìÅ Serving files from: {site_dir}\")\n",
    "        return server_url\n",
    "\n",
    "# Initialize the Web Acquisition Agent\n",
    "web_agent = WebAcquisitionAgent()\n",
    "print(\"ü§ñ Web Acquisition Agent (Auto-Port + Fixed) initialized!\")\n",
    "\n",
    "# Note about React limitations\n",
    "print(\"\\nüí° Note: Static server works for captured React content!\")\n",
    "print(\"   ‚úÖ Visual layout preserved (perfect for Marketing edits)\")\n",
    "print(\"   ‚ùå Interactive features disabled (buttons, forms won't work)\")\n",
    "print(\"   üéØ Ideal for HTML/CSS modifications and design changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing complete website capture: https://sportdev.us/\n",
      "üîç Web Acquisition Agent: Capturing https://sportdev.us/ with full structure\n",
      "   üåê Navigating to https://sportdev.us/\n",
      "   üìÑ Extracting HTML content\n",
      "   üé® Downloading assets\n",
      "   üìä Analyzing page structure\n",
      "   ‚úÖ Thread scraping complete\n",
      "‚úÖ Website captured with structure at: ../sites/sportdev.us_\n",
      "   üìÑ HTML: ../sites/sportdev.us_/index.html\n",
      "   üìä Analysis: ../sites/sportdev.us_/analysis.json\n",
      "   üé® Assets: 6 CSS, 9 JS, 9 images\n",
      "\n",
      "üìä Capture Results:\n",
      "   üìÑ HTML saved to: ../sites/sportdev.us_/index.html\n",
      "   üìÅ Site directory: ../sites/sportdev.us_\n",
      "\n",
      "üìà Analysis Summary:\n",
      "   Title: Sport.Dev\n",
      "   Headings: 15 found\n",
      "   Navigation: 4 items\n",
      "   CTAs: 1 found\n",
      "   Forms: 0 found\n",
      "   Images: 10 found\n",
      "\n",
      "üé® Assets Downloaded:\n",
      "   CSS files: 6\n",
      "   JS files: 9\n",
      "   Images: 9\n",
      "\n",
      "üåê Starting local preview server...\n",
      "üåê Starting server on port 8000\n",
      "Serving HTTP on :: port 8000 (http://[::]:8000/) ...\n",
      "üåê Local server started at http://localhost:8000\n",
      "üìÅ Serving files from: ../sites/sportdev.us_\n",
      "\n",
      "‚úÖ Complete website capture successful!\n",
      "üîó View at: http://localhost:8000\n",
      "üìù Marketing can now edit: ../sites/sportdev.us_/index.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "::1 - - [04/Jul/2025 01:14:12] \"GET / HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /css/style_0.css HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /css/style_1.css HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /css/style_3.css HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /css/style_4.css HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /js/script_1.js HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /css/style_2.css HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /css/style_5.css HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /js/script_3.js HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /js/script_2.js HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /js/script_4.js HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /js/script_5.js HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /js/script_6.js HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /js/script_0.js HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /images/image_0.png HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /images/image_1.jpg HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /images/image_3.jpg HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /images/image_4.jpg HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /images/image_5.jpg HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /js/script_7.js HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /js/script_8.js HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /images/image_7.png HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /images/image_8.jpg HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /images/image_9.jpg HTTP/1.1\" 200 -\n",
      "::1 - - [04/Jul/2025 01:14:12] \"GET /images/image_10.png HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "## Test Complete Website Capture with Structure Preservation\n",
    "\n",
    "# Test the enhanced Web Acquisition Agent\n",
    "url = 'https://sportdev.us/'\n",
    "print(f\"üß™ Testing complete website capture: {url}\")\n",
    "\n",
    "# Capture website with full structure\n",
    "capture_result = web_agent.fetch_website(url)\n",
    "\n",
    "if capture_result:\n",
    "    print(\"\\nüìä Capture Results:\")\n",
    "    print(f\"   üìÑ HTML saved to: {capture_result['html_path']}\")\n",
    "    print(f\"   üìÅ Site directory: {capture_result['site_dir']}\")\n",
    "    \n",
    "    # Show analysis summary\n",
    "    analysis = capture_result['analysis']\n",
    "    print(f\"\\nüìà Analysis Summary:\")\n",
    "    print(f\"   Title: {analysis.get('title', 'N/A')}\")\n",
    "    print(f\"   Headings: {len(analysis.get('headings', []))} found\")\n",
    "    print(f\"   Navigation: {len(analysis.get('navigation', []))} items\")\n",
    "    print(f\"   CTAs: {len(analysis.get('calls_to_action', []))} found\")\n",
    "    print(f\"   Forms: {len(analysis.get('forms', []))} found\")\n",
    "    print(f\"   Images: {len(analysis.get('images', []))} found\")\n",
    "    \n",
    "    # Show assets downloaded\n",
    "    assets = capture_result['assets']\n",
    "    print(f\"\\nüé® Assets Downloaded:\")\n",
    "    print(f\"   CSS files: {len(assets['css'])}\")\n",
    "    print(f\"   JS files: {len(assets['js'])}\")\n",
    "    print(f\"   Images: {len(assets['images'])}\")\n",
    "    \n",
    "    # Start local server for preview\n",
    "    print(f\"\\nüåê Starting local preview server...\")\n",
    "    server_url = web_agent.start_local_server(capture_result['site_dir'])\n",
    "    \n",
    "    print(f\"\\n‚úÖ Complete website capture successful!\")\n",
    "    print(f\"üîó View at: {server_url}\")\n",
    "    print(f\"üìù Marketing can now edit: {capture_result['html_path']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Failed to capture website structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Analysis Agent initialized!\n"
     ]
    }
   ],
   "source": [
    "## üß† Agent 2: Analysis Agent\n",
    "\n",
    "class AnalysisAgent:\n",
    "    \"\"\"AI-powered analysis of website design patterns and UX issues\"\"\"\n",
    "    \n",
    "    def __init__(self, anthropic_client):\n",
    "        self.client = anthropic_client\n",
    "    \n",
    "    def analyze_website(self, website_data):\n",
    "        \"\"\"Analyze website structure and identify design patterns and issues\"\"\"\n",
    "        print(\"üß† Analysis Agent: Processing website data...\")\n",
    "        \n",
    "        # Prepare structured data for AI analysis\n",
    "        analysis_prompt = f\"\"\"\n",
    "        You are a senior UX/UI analyst reviewing a website for a Marketing team. \n",
    "        Analyze the following website data and provide insights on:\n",
    "        1. Overall design patterns and user experience\n",
    "        2. Marketing effectiveness (messaging, CTAs, user journey)\n",
    "        3. Accessibility and usability concerns\n",
    "        4. Content hierarchy and information architecture\n",
    "        \n",
    "        Website Data:\n",
    "        Title: {website_data['title']}\n",
    "        Meta Description: {website_data['meta_description']}\n",
    "        \n",
    "        Headings Structure:\n",
    "        {json.dumps(website_data['headings'], indent=2)}\n",
    "        \n",
    "        Navigation Items: {website_data['navigation'][:10]}\n",
    "        \n",
    "        Call-to-Action Buttons: {website_data['calls_to_action']}\n",
    "        \n",
    "        Forms Present: {len(website_data['forms'])} forms detected\n",
    "        \n",
    "        Images: {len(website_data['images'])} images found\n",
    "        \n",
    "        Content Sections: {len(website_data['content_sections'])} main content areas\n",
    "        \n",
    "        Provide a structured analysis in the following format:\n",
    "        \n",
    "        DESIGN PATTERNS:\n",
    "        - [Key design patterns observed]\n",
    "        \n",
    "        MARKETING EFFECTIVENESS:\n",
    "        - [Assessment of messaging and conversion elements]\n",
    "        \n",
    "        USER EXPERIENCE ISSUES:\n",
    "        - [Potential usability problems]\n",
    "        \n",
    "        ACCESSIBILITY CONCERNS:\n",
    "        - [Basic accessibility observations]\n",
    "        \n",
    "        INFORMATION ARCHITECTURE:\n",
    "        - [Content organization and hierarchy assessment]\n",
    "        \n",
    "        Keep your analysis concise but actionable for a Marketing team.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=1000,\n",
    "                temperature=0.3,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": analysis_prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            analysis_result = response.content[0].text\n",
    "            print(\"‚úÖ Website analysis complete!\")\n",
    "            return analysis_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in analysis: {str(e)}\")\n",
    "            return f\"Analysis failed: {str(e)}\"\n",
    "\n",
    "# Initialize the Analysis Agent  \n",
    "analysis_agent = AnalysisAgent(client)\n",
    "print(\"üß† Analysis Agent initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Design Modification Agent initialized!\n"
     ]
    }
   ],
   "source": [
    "## üí° Agent 3: Design Modification Agent (HTML Structure Editing)\n",
    "\n",
    "class DesignModificationAgent:\n",
    "    \"\"\"Generates and applies specific HTML/CSS modifications for Marketing team\"\"\"\n",
    "    \n",
    "    def __init__(self, anthropic_client):\n",
    "        self.client = anthropic_client\n",
    "    \n",
    "    def generate_modifications(self, website_data, analysis_result, modification_request):\n",
    "        \"\"\"Generate specific HTML/CSS modifications based on Marketing request\"\"\"\n",
    "        print(\"üí° Design Modification Agent: Creating specific edits...\")\n",
    "        \n",
    "        modification_prompt = f\"\"\"\n",
    "        You are a Marketing-focused web designer. Based on the website analysis and Marketing request,\n",
    "        generate specific HTML/CSS modifications that can be applied to the existing website structure.\n",
    "        \n",
    "        Website Analysis:\n",
    "        {analysis_result}\n",
    "        \n",
    "        Current Website Structure:\n",
    "        - Title: {website_data['title']}\n",
    "        - Headings: {len(website_data.get('headings', []))} found\n",
    "        - CTAs: {len(website_data.get('calls_to_action', []))} found\n",
    "        - Navigation: {len(website_data.get('navigation', []))} items\n",
    "        \n",
    "        Marketing Request: {modification_request}\n",
    "        \n",
    "        Provide specific modifications in this format:\n",
    "        \n",
    "        üéØ RECOMMENDED MODIFICATIONS:\n",
    "        \n",
    "        1. [MODIFICATION TYPE] - [SPECIFIC CHANGE]\n",
    "           Target: [CSS selector or HTML element]\n",
    "           Change: [Exact CSS or HTML modification]\n",
    "           Reason: [Why this improves marketing effectiveness]\n",
    "           \n",
    "        2. [Continue for 2-3 modifications]\n",
    "        \n",
    "        üîß CSS MODIFICATIONS:\n",
    "        ```css\n",
    "        /* Specific CSS rules to add/modify */\n",
    "        ```\n",
    "        \n",
    "        üìù HTML MODIFICATIONS:\n",
    "        ```html\n",
    "        <!-- Specific HTML changes -->\n",
    "        ```\n",
    "        \n",
    "        üí° IMPLEMENTATION NOTES:\n",
    "        - [Step-by-step instructions for Marketing team]\n",
    "        \n",
    "        Keep modifications focused on high-impact marketing improvements.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=1500,\n",
    "                temperature=0.3,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": modification_prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            modifications = response.content[0].text\n",
    "            print(\"‚úÖ Design modifications generated!\")\n",
    "            return modifications\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating modifications: {str(e)}\")\n",
    "            return f\"Modification generation failed: {str(e)}\"\n",
    "\n",
    "    def apply_modifications(self, html_path, modifications_text):\n",
    "        \"\"\"Apply generated modifications to the HTML file\"\"\"\n",
    "        print(\"üîß Design Modification Agent: Applying changes...\")\n",
    "        \n",
    "        try:\n",
    "            # Read current HTML\n",
    "            with open(html_path, 'r', encoding='utf-8') as f:\n",
    "                html_content = f.read()\n",
    "            \n",
    "            # Create backup\n",
    "            backup_path = html_path.replace('.html', '_backup.html')\n",
    "            with open(backup_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(html_content)\n",
    "            \n",
    "            # Extract CSS modifications from the AI response\n",
    "            css_pattern = r'```css\\n(.*?)\\n```'\n",
    "            css_matches = re.findall(css_pattern, modifications_text, re.DOTALL)\n",
    "            \n",
    "            # Extract HTML modifications from the AI response  \n",
    "            html_pattern = r'```html\\n(.*?)\\n```'\n",
    "            html_matches = re.findall(html_pattern, modifications_text, re.DOTALL)\n",
    "            \n",
    "            modified_html = html_content\n",
    "            \n",
    "            # Apply CSS modifications\n",
    "            if css_matches:\n",
    "                css_modifications = '\\n'.join(css_matches)\n",
    "                # Add CSS to the head section\n",
    "                if '</head>' in modified_html:\n",
    "                    css_block = f'\\n<style>\\n/* Marketing Modifications */\\n{css_modifications}\\n</style>\\n'\n",
    "                    modified_html = modified_html.replace('</head>', css_block + '</head>')\n",
    "                else:\n",
    "                    # Add CSS at the beginning of the document\n",
    "                    css_block = f'<style>\\n/* Marketing Modifications */\\n{css_modifications}\\n</style>\\n'\n",
    "                    modified_html = css_block + modified_html\n",
    "            \n",
    "            # Apply HTML modifications (this would need more sophisticated parsing)\n",
    "            # For now, we'll just add the modifications as comments for Marketing team review\n",
    "            if html_matches:\n",
    "                html_modifications = '\\n'.join(html_matches)\n",
    "                modification_comment = f'\\n<!-- MARKETING MODIFICATIONS TO APPLY:\\n{html_modifications}\\n-->\\n'\n",
    "                modified_html = modification_comment + modified_html\n",
    "            \n",
    "            # Save modified HTML\n",
    "            with open(html_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(modified_html)\n",
    "            \n",
    "            print(f\"‚úÖ Modifications applied!\")\n",
    "            print(f\"   üìÑ Modified: {html_path}\")\n",
    "            print(f\"   üíæ Backup: {backup_path}\")\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'backup_path': backup_path,\n",
    "                'css_applied': len(css_matches) > 0,\n",
    "                'html_notes': len(html_matches) > 0\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error applying modifications: {str(e)}\")\n",
    "            return {'success': False, 'error': str(e)}\n",
    "\n",
    "# Initialize the Design Modification Agent\n",
    "modification_agent = DesignModificationAgent(client)\n",
    "print(\"üí° Design Modification Agent initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Demo Value Proposition\n",
    "\n",
    "**What the team just witnessed:**\n",
    "\n",
    "‚úÖ **Multi-Agent System**: 3 specialized AI agents working in coordination  \n",
    "‚úÖ **Real Website Analysis**: Live scraping and analysis of any website  \n",
    "‚úÖ **AI-Powered Insights**: Advanced analysis using Claude 3.5 Sonnet  \n",
    "‚úÖ **Marketing-Focused Output**: Actionable recommendations for Marketing teams  \n",
    "‚úÖ **30-Second Execution**: Rapid analysis that scales to any website  \n",
    "\n",
    "**Immediate Business Value:**\n",
    "- **Time Savings**: Replace hours of manual analysis with 30-second automated insights\n",
    "- **Marketing Autonomy**: Enable Marketing team to analyze competitor sites independently  \n",
    "- **Consistent Analysis**: Standardized evaluation framework across all websites\n",
    "- **Actionable Output**: Specific recommendations, not just generic observations\n",
    "- **Scalability**: Analyze hundreds of websites with the same effort as one\n",
    "\n",
    "**Next Steps:**\n",
    "1. Expand to local website rendering and modification\n",
    "2. Add A/B testing and design option generation  \n",
    "3. Build user-friendly interface for non-technical users\n",
    "4. Integrate with existing Marketing tools and workflows\n",
    "\n",
    "*This demo represents Day 1 of the 2-week development sprint. The foundation is solid and ready for rapid expansion!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ AGENTIC WEBSITE ANALYSIS DEMO\n",
      "==================================================\n",
      "Available demo URLs:\n",
      "  1. https://www.familysearch.org/en/campaign/temple-ord-ready\n",
      "  2. https://anthropic.com\n",
      "  3. https://github.com\n",
      "  4. https://stripe.com\n",
      "\n",
      "To run demo after all agents are initialized:\n",
      "  orchestrator.run_complete_analysis('URL_HERE')\n",
      "==================================================\n",
      "üí° Quick demo function available: quick_demo(1) through quick_demo(4)\n"
     ]
    }
   ],
   "source": [
    "# üöÄ LIVE DEMO - Try It Now!\n",
    "\n",
    "# Demo Configuration\n",
    "DEMO_URLS = [\n",
    "    \"https://www.familysearch.org/en/campaign/temple-ord-ready\",  # Simple, reliable test site\n",
    "    \"https://anthropic.com\",  # AI company with modern design\n",
    "    \"https://github.com\",  # Popular developer platform\n",
    "    \"https://stripe.com\",  # Clean, conversion-focused design\n",
    "]\n",
    "\n",
    "print(\"üöÄ AGENTIC WEBSITE ANALYSIS DEMO\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Available demo URLs:\")\n",
    "for i, url in enumerate(DEMO_URLS, 1):\n",
    "    print(f\"  {i}. {url}\")\n",
    "print(\"\\nTo run demo after all agents are initialized:\")\n",
    "print(\"  orchestrator.run_complete_analysis('URL_HERE')\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Quick demo function for easier testing\n",
    "def quick_demo(url_index=1):\n",
    "    \"\"\"Run demo with one of the predefined URLs\"\"\"\n",
    "    if url_index < 1 or url_index > len(DEMO_URLS):\n",
    "        print(f\"‚ùå Invalid URL index. Choose 1-{len(DEMO_URLS)}\")\n",
    "        return\n",
    "    \n",
    "    url = DEMO_URLS[url_index - 1]\n",
    "    try:\n",
    "        return orchestrator.run_complete_analysis(url)\n",
    "    except NameError:\n",
    "        print(\"‚ùå Error: Please run all agent initialization cells first!\")\n",
    "        print(\"   Required execution order:\")\n",
    "        print(\"   1. Setup cell (imports and dependencies)\")\n",
    "        print(\"   2. Web Acquisition Agent\")\n",
    "        print(\"   3. Analysis Agent\") \n",
    "        print(\"   4. Design Suggestion Agent\")\n",
    "        print(\"   5. Demo Orchestrator\")\n",
    "        print(\"   6. Then you can run demos!\")\n",
    "        return None\n",
    "\n",
    "print(\"üí° Quick demo function available: quick_demo(1) through quick_demo(4)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
